{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937f113a-a681-4598-b7c8-e2f823485001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class pMHCDataset(Dataset):\n",
    "    def __init__(self, root, filename, aaindex, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        self.aaindex = aaindex\n",
    "        super(pMHCDataset, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "            (The download func. is not implemented here)  \n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in processed_dir, processing is skipped\"\"\"\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "        return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "\n",
    "    def download(self):\n",
    "        pass##不需要下载\n",
    "    \n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0])\n",
    "        for index, sample in tqdm(self.data.iterrows(), total=self.data.shape[0]):#tqdm可以显示运行进程\n",
    "            # Get node features\n",
    "            node_feats = self._get_node_features(sample[\"pep\"],sample[\"hla_seq\"],self.aaindex)\n",
    "            edge_index = self._get_edge_index(sample[\"pep\"],sample[\"hla_seq\"])\n",
    "            label = self._get_labels(sample[\"type\"])\n",
    "            # Create data object\n",
    "            data = Data(x=node_feats, edge_index=edge_index, y=label, index=0) \n",
    "            torch.save(data, os.path.join(self.processed_dir, f'data_{index}.pt'))\n",
    "\n",
    "    def _get_node_features(self, pep, HLA, aaindex):\n",
    "        \"\"\" \n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        all_seq = pep + HLA\n",
    "        all_node_feats = []\n",
    "        for index, aa in enumerate(all_seq):\n",
    "            node_feats = []\n",
    "            ##aaindex\n",
    "            node_feats.extend(aaindex[aa].to_list())\n",
    "            anchar = [0,len(pep)]\n",
    "            seq_onehot = [0,0]\n",
    "            seq_onehot[sum([index >= i for i in anchar])-1] = 1\n",
    "            node_feats.extend(seq_onehot)\n",
    "            all_node_feats.append(node_feats)\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats)\n",
    "        \n",
    "    \n",
    "    def _get_labels(self, label):\n",
    "        label = np.asarray([label])\n",
    "        return torch.tensor(label)\n",
    "    \n",
    "    def _get_edge_index(self, pep, hla):\n",
    "        ##生成边\n",
    "        nodes = list(range(0,len(pep)+len(hla)))\n",
    "        edge_index = [[],[]]  \n",
    "        for i,_ in enumerate(pep):\n",
    "            nodes_cp = copy.deepcopy(nodes)\n",
    "            nodes_cp.remove(i)\n",
    "            edge_index[0].extend([i]*(len(nodes)-1))\n",
    "            edge_index[1].extend(nodes_cp)\n",
    "        for i,_ in enumerate(hla):\n",
    "            i = i + len(pep)\n",
    "            nodes_cp = copy.deepcopy(nodes)\n",
    "            nodes_cp.remove(i)\n",
    "            edge_index[0].extend([i]*(len(nodes)-1))\n",
    "            edge_index[1].extend(nodes_cp)  \n",
    "        edge_index = torch.tensor(edge_index)\n",
    "        return edge_index\n",
    "    \n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt')) \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627cd815-8196-4c8d-aed9-54b35a2e01de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aaindex = pd.read_csv(\"../model/aaindex1_pca.csv\")\n",
    "train_dt = pMHCDataset(root=\"/home/data/sda/wt/Neodb_model/\",\n",
    "                       filename=\"train_data_iedb_2.csv\",\n",
    "                       aaindex=aaindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7b701a-ea80-4bc1-8049-f8e5ced347ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import TransformerConv,  GraphNorm\n",
    "from torch.nn import Linear, ModuleList, LeakyReLU\n",
    "from gtrick.pyg import VirtualNode\n",
    "from torch.nn import LeakyReLU\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, feature_size, model_params):\n",
    "        super().__init__()\n",
    "        \n",
    "        embedding_size = model_params[\"model_embedding_size\"]\n",
    "        dense_neurons = model_params[\"model_dense_neurons\"]\n",
    "        n_heads = model_params[\"model_heads\"]\n",
    "        n_layers = model_params[\"model_layers\"]\n",
    "        self.n_layers = n_layers\n",
    "        self.top_k_every_n = 1\n",
    "        self.conv_layers = ModuleList([])\n",
    "        self.transf_layers = ModuleList([])\n",
    "        self.pooling_layers = ModuleList([])\n",
    "        self.bn_layers = ModuleList([])\n",
    "        self.vns = ModuleList()\n",
    "        self.relu = LeakyReLU()\n",
    "\n",
    "        # Transformation layer\n",
    "        self.conv1 = TransformerConv(feature_size, \n",
    "                                    embedding_size, \n",
    "                                    heads=n_heads,\n",
    "                                    beta=True) \n",
    "\n",
    "        self.transf1 = Linear(embedding_size*n_heads, embedding_size)\n",
    "        self.bn1 =  GraphNorm(embedding_size)\n",
    "        # Other layers\n",
    "        for i in range(n_layers):\n",
    "            self.conv_layers.append(TransformerConv(embedding_size, \n",
    "                                                    embedding_size, \n",
    "                                                    heads=n_heads,\n",
    "                                                    beta=True))\n",
    "\n",
    "            self.transf_layers.append(Linear(embedding_size*n_heads, embedding_size))\n",
    "            self.bn_layers.append(GraphNorm(embedding_size))\n",
    "            self.vns.append(VirtualNode(embedding_size, embedding_size))\n",
    "            \n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = Linear(embedding_size, dense_neurons)\n",
    "        self.linear2 = Linear(dense_neurons, 1)  \n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # Initial transformation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(self.transf1(x))\n",
    "        x = self.bn1(x, batch_index)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            x, vx = self.vns[i].update_node_emb(x, edge_index, batch_index)\n",
    "            if i == 2:\n",
    "                x, (edge, attention_weights) = self.conv_layers[i](x, edge_index, return_attention_weights=True)\n",
    "            else:\n",
    "                x = self.conv_layers[i](x, edge_index)\n",
    "            x = self.relu(self.transf_layers[i](x))\n",
    "            x = self.bn_layers[i](x, batch_index)\n",
    "            vx = self.vns[i].update_vn_emb(x, batch_index, vx)\n",
    "        \n",
    "        # Output block\n",
    "        x = self.relu(self.linear1(vx))\n",
    "        x = self.linear2(x)\n",
    "        return x, (edge, attention_weights)\n",
    "\n",
    "HYPERPARAMETERS = {\n",
    "    \"model_embedding_size\": 64, \n",
    "    \"model_dense_neurons\": 32,\n",
    "    \"model_heads\":3,\n",
    "    \"model_layers\":3\n",
    "}\n",
    "model_params = {k: v for k, v in HYPERPARAMETERS.items() if k.startswith(\"model_\")}\n",
    "model = GNN(feature_size=22, model_params=model_params) \n",
    "model_file = \"last_model.pt\"\n",
    "model.load_state_dict(torch.load(model_file,map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "001701ca-5376-4e06-80ae-0c5c52d5a1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8412"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34af38ba-0c35-442e-91be-d93b04214aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "data_loader = DataLoader(train_dt, batch_size=8412, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc18956-e736-48b0-9db6-61fb06a501e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for _, batch in enumerate(tqdm(data_loader)):\n",
    "    input_x = batch.x\n",
    "    edge_index = batch.edge_index\n",
    "    batch_index = batch.batch\n",
    "    model.eval()\n",
    "    pred, (edge_index, weight) = model(input_x.float(), edge_index, batch_index)\n",
    "    #weight_mean = torch.max(weight,dim=1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d722578-d8b4-41e4-baa3-b2f98ce219b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_mean = torch.max(weight,dim=1).values.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf954b-e19c-4be9-a709-2e3deb427659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edge_index = edge_index.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e2427-fcfc-4879-8f50-2795a853081c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edge_index.shape,weight_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a208c-e703-43d9-8e89-95ed763e3ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad1b22-8f8b-45bf-bdc9-a2eb5469f8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'index1': edge_index[0, :], 'index2': edge_index[1, :]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e7a2d-cb26-4a9d-9027-1ae6548ccf12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/data/sda/wt/model_data/Neodb_all_edge_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876013d9-f972-40df-ad54-e4101097bc4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_mean = pd.DataFrame({\"weight_mean\":weight_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c62c8-926f-481b-987d-d9c561ae2206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_mean.to_csv(\"/home/data/sda/wt/model_data/Neodb_all_weight_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca7442-61f2-4cf4-bd55-dc080220595c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
